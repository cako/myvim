\documentclass[mathserif]{beamer}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{units}
\usepackage{color}
\let\Tiny=\tiny


\newcommand{\dif}{\,\text{d}}
\newcommand{\ut}[1]{\underset{\widetilde{}}{\mathbf{#1}}\phantom{}}
\newcommand{\fixutspace}{\\ \vspace{-1.25mm}}

\usetheme{Darmstadt}

\title[Convolutional quelling]{Convolutional quelling in seismic tomography \\
\cite{MPS}}
\author{Carlos Alberto da Costa Filho}
\institute{IMECC -- Unicamp}
\date{\today}

%\setbeamertemplate{caption}[numbered]
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
    \tableofcontents
\end{frame}

\section{Problema}
\subsection{Geral}
\begin{frame}
    \frametitle{O problema geral}
    \begin{itemize}
        \item A tomografia sísmica consiste em estimar propriedades físicas da terra.
        \pause
        \vfill\item Uma propriedade comum é a velocidade de propagação de ondas P.
        \pause
        \vfill\item É difícil controlar a regularidade da obtenção de dados.
        \pause
        \vfill\item Os dados têm ruídos e podem ser irregulares, diferente de tomografia computadorizada, onde a geometria dos receptores e fontes pode ser controlada.
        \pause
        \vfill\item Estes erros geram ``artefatos'' na imagem que podem ser mal interpretados (veremos exemplos).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Teoria}
    \begin{itemize}
        \item Área com distribuição de vagarosidade desconhecida.
        \pause
        \vfill\item Tomografia de tempo de trânsito: buscamos velocidades em função da posição usando o tempo de trânsito das ondas.
        \pause
        \vfill\item Podemos relacionar $M$ tempos de trânsito com a vagarosidade da seguinte forma:
        \begin{equation}\label{eq:integral}
            t_i = \int_{\Gamma_i} u(r)\dif s
        \end{equation}
        \pause
        Temos equações não lineares: 
        \begin{itemize}
            \item $t_i =$ tempo de trânsito $i$;
            \item $u(r) =$ vagarosidade em função da posição;
            \item $\Gamma_i =$ caminho percorrido pelo raio $i$ (depende de $u$).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Simplificações}
    \begin{itemize}
        \item Vamos linearizar $u(r)$ na equação \ref{eq:integral}.
            \begin{equation}\label{eq:traveltime}
                t_i = \int_{\Gamma_i}u_0(r)\dif s + \int_{\Gamma_i} \delta u(r)\dif s
            \end{equation}
        \pause
        \vfill\item Com $u_0(r) \equiv u_0$ constante, e os caminhos $\Gamma_i$
        sendo retas, reescrevemos a equação \ref{eq:traveltime}:
            \begin{equation}\label{eq:resid_cont}
                d_i \equiv \int_{\Gamma_i} \delta u(r)\dif s = t_i - \int_{\Gamma_i} u_0 \dif s
            \end{equation}
        \pause
        \vfill\item A teoria de raios é válida.
            \begin{itemize}
                \item A frente de onda se comporta como um raio.
                \item Não há refração.
                \item Não há difração.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Exemplo}
    Vamos dividir a área em células discretas nas quais a velocidade é constante.
    Em seguida, vamos atravessar raios pela região e medir seu tempo de trânsito.
    \begin{columns}
        \begin{column}{0.35\linewidth}
            \begin{tikzpicture}[scale=1]
                \draw (0, 0) grid (3, 3);
                \draw [|-|] (2, 3.2) -- node[above]{\scriptsize $h$} (3, 3.2);
                \draw [|-|] (3.2, 3) -- node[right]{\scriptsize $h$} (3.2, 2);
                \begin{scope}[pink]
                    \filldraw (0.5, 0) circle (1.5pt);
                    \filldraw (1.5, 0) circle (1.5pt);
                    \filldraw (2.5, 0) circle (1.5pt);
                    \filldraw (3, 0.5) circle (1.5pt);
                    \filldraw (3, 1.5) circle (1.5pt);
                    \filldraw (3, 2.5) circle (1.5pt);
                \end{scope}
                \begin{scope}[black]
                    \filldraw (0, 2.5) circle (1.5pt);
                    \draw [semithick, ->] (0, 2.5) -- (3, 2.5);
                    \filldraw (0.5, 3) circle (1.5pt);
                    \draw [semithick, ->] (0.5, 3) -- (0.5, 0);
                \end{scope}
                \begin{scope}[blue]
                    \filldraw (0, 1.5) circle (1.5pt);
                    \draw [semithick, ->] (0, 1.5) -- (3, 1.5);
                    \filldraw (1.5, 3) circle (1.5pt);
                    \draw [semithick, ->] (1.5, 3) -- (1.5, 0);
                \end{scope}
                \begin{scope}[red]
                    \filldraw (0, 0.5) circle (1.5pt);
                    \draw [semithick, ->] (0, 0.5) -- (3, 0.5);
                    \filldraw (2.5, 3) circle (1.5pt);
                    \draw [semithick, ->] (2.5, 3) -- (2.5, 0);
                \end{scope}
                \draw[white,very thin] (0.2, 2.25) -- node[above,black,fill=white]{\tiny $\tilde{m}_1$} (0.8, 2.25);
                \draw[gray, very thin] (0.2, 2.25) rectangle (0.8, 2.69);
                \draw[white,very thin] (1.2, 2.25) -- node[above,black,fill=white]{\tiny $\tilde{m}_2$} (1.8, 2.25);
                \draw[gray, very thin] (1.2, 2.25) rectangle (1.8, 2.69);
                \draw[white,very thin] (2.2, 0.25) -- node[above,black,fill=white]{\tiny $\tilde{m}_9$} (2.8, 0.25);
                \draw[gray, very thin] (2.2, 0.25) rectangle (2.8, 0.69);
            \end{tikzpicture}
        \end{column}
        \pause
        \begin{column}{0.65\linewidth}
            \small
            \begin{equation}\label{eq:ex}
                h\begin{bmatrix}
1 & 1               & 1               &                 &                 &                &   &                 &                \\
  & {\color{blue}1} & {\color{blue}1} & {\color{blue}1} &                 &                &   &                 &                \\
  &                 & {\color{red}1}  & {\color{red}1}  & {\color{red}1}  &                &   &                 &                \\
1 &                 &                 & 1               &                 &                & 1 &                 &                \\
  & {\color{blue}1} &                 &                 & {\color{blue}1} &                &   & {\color{blue}1} &                \\
  &                 & {\color{red}1}  &                 &                 & {\color{red}1} &   &                 & {\color{red}1} \\
                \end{bmatrix}
                \begin{bmatrix}
                    \tilde{m}_1\\
                    \tilde{m}_2\\
                    \tilde{m}_3\\
                    \tilde{m}_4\\
                    \tilde{m}_5\\
                    \tilde{m}_6\\
                    \tilde{m}_7\\
                    \tilde{m}_8\\
                    \tilde{m}_9\\
                \end{bmatrix}
                =
                \begin{bmatrix}
                    t_1\\
                    t_2\\
                    t_3\\
                    t_4\\
                    t_5\\
                    t_6\\
                \end{bmatrix}
            \end{equation}   
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Problema discretizado}
    \begin{itemize}
        \item Podemos descrever o problema como encontrar $\mathbf{\tilde{m}}$ tal que
        \begin{equation}\label{eq:naoreduz}
            \ut{G}\mathbf{\tilde{m}} = \mathbf{t}.
        \end{equation}
        \pause
        \item Este é o problema não reduzido. Não vamos fazer isso.
        \pause
        \vfill\item Lembre que temos uma aproximação $u_0$ para $u(r)$.
        Vamos substituir $\tilde{m}_i$ por $m_i + u_0$, onde $m_i$ é a vagarosidade relativa à velocidade de fundo $u_0$.
        \begin{equation}\label{eq:resid_disc}
            \ut{G}\mathbf{m} = \mathbf{t} - \ut{G}\mathbf{u_0} 
        \end{equation}
        \pause
        \vfill\item Vamos comparar com a equação \ref{eq:resid_cont}
        \[d_i \equiv \int_{\Gamma_i} \delta u(r)\dif s = t_i - \int_{\Gamma_i} u_0 \dif s\]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Problema discretizado}
    \begin{itemize}
        \item Definindo o vetor $\mathbf{d} = \mathbf{t} - \ut{G}\mathbf{u_0}$,
        o problema é encontrar $\mathbf{m}$ tal que 
        \begin{equation}\label{eq:reduz}
            \ut{G}\mathbf{m} = \mathbf{d}.
        \end{equation}
        \pause
        \vspace{-0.8cm}\item Observações:
        \begin{itemize}
            \item Fizemos um pequeno exemplo, mas esta é a forma típica do
            problema \cite{Menke84b}.
            \pause
            \item O vetor $\mathbf{d}$ é o vetor de tempos de trânsito residuais.
            \pause
            \item O vetor $\mathbf{m}$ é o vetor de vagarosidades relativas à $u_0$.
            \pause
            \item A matriz $\ut{G}$ tem tamanho $M\times N$, onde $M$ é o número de\\ \vspace{-1.25mm}tempos de trânsito (ou de raios)
            e $N$ é o número de células. Em problemas reais $M>N$.
            Seus elementos $G_{ij}$ são os comprimentos dos segmentos dos raios $\Gamma_i$
            que atravessam a célula $j$. A matrix $\ut{G}$ é esparsa já que cada raio atravessa \\ \vspace{-1.25mm} apenas um número pequeno de células.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Específico}
\begin{frame}
    \frametitle{Modelos sintéticos}
    \begin{itemize}
        \item Um modelo sintético é uma coleção de dados manufaturados.
        \pause
        \vfill\item Modelos sintéticos são preferíveis para fins de estudo já que o
        resultado pode ser comparado com o modelo verdadeiro.
        \pause
        \vfill\item Depois de testar com modelos sintéticos, pode-se testar com dados reais. Não faremos isso.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nosso modelo}
    Usaremos o seguinte modelo sintético.
    \begin{columns}
        \begin{column}{0.62\linewidth}
            \begin{figure}[h]
                \includegraphics[height=5.0cm]{model.png}
                \caption{Modelo sintético usado nas simulações \cite{MPS}.}
            \end{figure}
        \end{column}
        \pause
        \begin{column}{0.38\linewidth}
        \vspace{-1cm}
            \begin{itemize}
                \item O fundo representa uma velocidade de \unitfrac[4572]{m}{s}.
                \pause
                \item As anomalias na velocidade seguem a escala acima da figura.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nosso modelo}
    Os tempos de trânsito foram calculados a partir dos seguintes raios:
    \begin{columns}
        \begin{column}{0.62\linewidth}
            \begin{figure}[h]
                \includegraphics[height=4.5cm]{rays.png}
                \caption{Geometria dos raios \cite{MPS}. Somente são mostrados um quarto dos raios.}
                
                \label{fig:rays}
            \end{figure}
        \end{column}
        \pause
        \begin{column}{0.38\linewidth}
        \vspace{-1cm}
            \begin{itemize}
                \item A região tem \unit[2.4]{km} por \unit[3.2]{km}.
                \pause
                \item Foi acrescido ruído Gaussiano com $\mu = 0$ e $\sigma =$ \unit[0.001]{s} 
                aos tempos de trânsito reduzidos.
                \pause
                \item Gera uma $\ut{G}$ que é \\ \vspace{-1.25mm} $1536\times 1900$ com somente $2\%$ das entradas não nulas.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\section{Resolução}
\subsection{Inversão}
\begin{frame}
    \frametitle{Inversão}
    \begin{itemize}
        \item A ideia é, a partir de $\ut{G}$, achar uma inversa generalizada
        $\ut{G}^{-g}$ \fixutspace tal que
        \begin{equation}\label{eq:inversao}
            \hat{\mathbf{m}} = \ut{G}^{-g}\mathbf{d}
        \end{equation}
        \pause
        \item O problema é indeterminado.
        \pause
        \item Temos poucas opções, já que qualquer resolução deve preservar
        a esparsidade.
        \pause
        \item Na hora de inverter a matriz, são usados métodos iterativos. No
        \emph{paper} usou-se um \emph{solver} que utiliza gradientes conjugados.
        \pause
        \item Veremos algumas soluções:
        \begin{itemize}
            \item Mínimos quadrados amortecidos (\emph{damped least squares}),
            \item Mínimos quadrados ponderados (\emph{weighted least squares}),
            \item Supressão convolucional (\emph{convolutional quelling}).
        \end{itemize}
    \end{itemize}
\end{frame}
\subsection{Mínimos quadrados}
\begin{frame}
    \frametitle{Mínimos quadrados}
    \begin{itemize}
        \item A solução de mínimos quadrados (LS) usual é
        \begin{equation}\label{eq:LS}
            \hat{\mathbf{m}} = (\ut{G}^T\ut{G})^{-1}\ut{G}^T\mathbf{d}.
        \end{equation}
        \pause
        \item Não funciona na maioria dos casos: $\ut{G}^T\ut{G}$ não
        é inversível, o \\ \vspace{-1.25mm} que é o mesmo que dizer que mais
        de uma solução minimiza a norma $L_2$ do erro:
        \[(\mathbf{d} - \ut{G}\mathbf{m})^T(\mathbf{d} - \ut{G}\mathbf{m}).\]
        \pause
        \item Vamos introduzir mais uma informação \emph{a priori} para
        que a solução seja única.
    \end{itemize}
\end{frame}

\subsection{Mínimos quadrados amortecidos}
\begin{frame}
    \frametitle{Mínimos quadrados amortecidos}
    \begin{itemize}
        \item Para encontrar a solução LS, minimizou-se a
        norma $L_2$ do erro, ou seja, minimizou-se
        $(\mathbf{d} - \ut{G}\mathbf{m})^T(\mathbf{d} - \ut{G}\mathbf{m})$.
        \vfill\item Agora, vamos buscar uma solução que minimiza uma
        combinação do erro e da norma $L_2$ da solução. Queremos minimizar
        \[\Phi(\mathbf{m}) = (\mathbf{d} - \ut{G}\mathbf{m})^T(\mathbf{d} -
        \ut{G}\mathbf{m}) + \theta^2\mathbf{m}^T\mathbf{m}\]
        \vfill\item Tal solução é dada por
        \begin{equation}\label{eq:DLS}
            \hat{\mathbf{m}} = (\ut{G}^T\ut{G} + \theta^2 \ut{I})^{-1}\mathbf{G}^T\mathbf{d}
        \end{equation}
        e é dita solução de mínimos quadrados amortecidos (DLS). Vejamos sua
        derivação.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Derivação}
    \[\Phi(\mathbf{m}) = \mathbf{d}^T\mathbf{d} - 2\mathbf{m}^T\ut{G}^T\mathbf{d} +
    \mathbf{m}^T\ut{G}^T\ut{G}\mathbf{m} + \theta^2\mathbf{m}^T\mathbf{m}\]
    \pause
    \[\frac{\partial \Phi}{\partial \mathbf{m}} = -2\ut{G}^T\mathbf{d} +
    2\ut{G}^T\ut{G}\mathbf{m} + 2\theta^2\mathbf{m}\]
    \pause
    \[\frac{\partial \Phi}{\partial \mathbf{m}} = 0 \Leftrightarrow
    \ut{G}^T\mathbf{d} = \ut{G}^T\ut{G}\mathbf{m} + \theta^2\mathbf{m}\]
    \pause
    Podemos, então, escrever a solução DLS da seguinte forma:
    \[\hat{\mathbf{m}} = (\ut{G}^T\ut{G} + \theta^2
    \ut{I})^{-1}\mathbf{G}^T\mathbf{d}.\]
    \pause
    Note que, é sempre possível escolher um $\theta^2$ tal que $\ut{G}^T\ut{G}
    + \theta^2 \ut{I}$ é \\ \vspace{-1.25mm} inversível. Isto é observado usando um argumento de
    autovalores.
\end{frame}

\subsection{Mínimos quadrados ponderados}
\begin{frame}
    \frametitle{Mínimos quadrados ponderados}
    \begin{itemize}
        \item As vezes, minimizar a norma $L_2$ do erro não é o objetivo.
        Podemos querer minimizar outra quantidade baseada em $\mathbf{m}$.
        \pause
        \vfill\item Podemos fazer isso usando uma matriz $\ut{W}$ que dá diferentes 
        \\ \vspace{-1.25mm} pesos aos componentes da solução, ou seja,
        fazendo as seguintes transformação de variáveis:
        \[\mathbf{m}^\prime = \ut{W}^{-1}\mathbf{m} \text{ e }\ut{G}^\prime = \ut{G}\ut{W}\]
        e depois aplicar DLS no sistema resultante.
        \pause
        \vfill\item A solução resultante é chamada de mínimos quadrados
        ponderados (WLS) e é dada por
        \begin{equation}\label{eq:WLS}
            \hat{\mathbf{m}} = \ut{W}(\ut{W}^T\ut{G}^T\ut{G}\ut{W} +
            \theta^2\ut{I})^{-1}\ut{W}^T\ut{G}^T\mathbf{d}.
        \end{equation}
        Vejamos sua derivação.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Derivação}
    Seja $\ut{W}$ uma matriz inversível $N \times N$. Podemos escrever o sistema
    da seguinte forma
    \[\mathbf{d} = \ut{G}\ut{W}\ut{W}^{-1}\mathbf{m}.\]
    \vfill
    \pause
    Fazendo as trocas de variáveis $\mathbf{m}^\prime = \ut{W}^{-1}\mathbf{m}$ e
    $\ut{G}^\prime = \ut{G}\ut{W}$, podemos escrever a equação acima da seguinte
    forma:
    \[\mathbf{d} = \ut{G}^\prime\mathbf{m}^\prime.\]
\end{frame}

\begin{frame}
    \frametitle{Derivação}
    Note que o sistema tem exatamente a mesma forma do nosso sistema
    anterior. Portanto, admite uma solução DLS:
    \[\hat{\mathbf{m}}^\prime = ({\ut{G}^\prime}^T\ut{G}^\prime +
    \theta^2\ut{I}){\ut{G}^\prime}^T\mathbf{d}.\]
    \vfill
    \pause
    Destrocando as variáveis, e aplicando $\ut{W}$ nos dois lados, chegamos na
    equação \ref{eq:WLS}:
    \[ \hat{\mathbf{m}} = \ut{W}(\ut{W}^T\ut{G}^T\ut{G}\ut{W} +
            \theta^2\ut{I})^{-1}\ut{W}^T\ut{G}^T\mathbf{d}.\]
\end{frame}

\subsection{Supressão convolucional}
\begin{frame}
    \frametitle{Supressão convolucional}
    \begin{itemize}
        \item Temos poucas opções para $\ut{W}$ já que deve preservar a
        \fixutspace esparsidade. Um opção óbvia é usar $\ut{W}$ diagonal.
        \pause
        \vfill\item A supressão convolucional (CQ) usa, em contraste, matrizes
        banda. Assim, ela adiciona somente poucos elementos não nulos em $\ut{G}$.
        \pause
        \vfill\item A teoria de supressão (\emph{quelling}) é mais geral.
        Contudo, a convolução, neste caso a convolução discreta é um tipo de
        supressão. (Ver apêndice).
        \pause
        \vfill\item Note que o mesmo \emph{solver} usado para DLS pode ser
        usado para achar a solução de CQ.
    \end{itemize}
\end{frame}

\section{Resultados}
%\subsection{Introdução}
%\newcommand{\truem}{\hat{\mathbf{m}}^\text{t}}
%\begin{frame}
    %\frametitle{Matriz de resolução}
    %\begin{itemize}
        %\item Suponha que existe uma solução verdadeira do problema,
    %$\truem$ que resolve $\ut{G}\truem = \mathbf{d}$.
        %\pause
        %\vfill\item Assim, a equação $\hat{\mathbf{m}} = \ut{G}^{-g}\mathbf{d}$
        %vira
        %\[ \hat{\mathbf{m}} = \ut{G}^{-g}\ut{G}\truem \]
        %\pause
        %\vfill\item Desta forma, quanto mais perto da identidade for a matriz
        %$\ut{R} = \ut{G}^{-g}\ut{G}$, melhor é a resolução.
        %\pause
        %\vfill\item Esta matriz tem tamanho $N\times N$ e é dita matriz de
        %resolução.
    %\end{itemize}
%\end{frame}

%\begin{frame}
    %\frametitle{Cálculo da matriz de resolução}
    %\begin{itemize}
        %\item Não temos a matriz $\ut{G}^{-g}$ para nenhum modelo, visto que a
        %solução é calculada iterativamente.
        %\pause
        %\vfill\item Calculam-se as \emph{response functions} em alguns pontos.
        %A $p$-ésima \emph{response function} é dada pela seguinte equação:
        %\[
        %\mathbf{r}_p = \ut{G}^{-g}\ut{G}\mathbf{e}_p
        %\] e equivale à $p$-ésima coluna de $R$.
    %\end{itemize}
%\end{frame}

%\begin{frame}
    %\frametitle{Cálculo da matriz de resolução}
    %\begin{itemize}
        %\item \emph{Resolving function}?
            %\pause
        %\vfill\item Se $\ut{R}$ é simétrica, $\mathbf{r}_p$ é equivalente à
        %\emph{resolving function} para \fixutspace qualquer escolha de $p$.
        %\pause
        %\vfill\item A matriz de resolução corresponde à solução DLS é simétrica.
        %\pause
        %\vfill\item Note que $\mathbf{r}_p$ pode não representar fielmente a
        %$p$-ésima \emph{response function}, já que é calculado iterativamente. Contudo,
        %em vista das limitações, é o melhor que se pode fazer.
    %\end{itemize}
%\end{frame}

\subsection{Especificações}
\begin{frame}
    \frametitle{Solução DLS}
    \begin{itemize}
        \item Uma malha de $58\times 38$ pixels resultou em um sistema $1536 \times
        1900$. A mesma ordem de gradeza nos números representa uma boa
        conciliação entre uma malha muito fina e muito grosseira. 
        \pause
        \vfill\item O pixel tem aproximadamente \unit[55]{m}$\times$\unit[63]{m}.
        \pause
        \vfill\item Foi usado um $\theta = $ \unit[450]{m}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Solução CQ}
    \begin{itemize}
        \item A $\ut{W}$ corresponde a um \emph{eight nearest neighbor
        smoother}. \fixutspace Na figura abaixo, os raios passam pelas células não tracejadas.
            \begin{figure}[h]
                \includegraphics[height=4.8cm]{smoother.png}
                \caption{Ilustração do processo de supressão convolucional.}
                \label{fig:smoother}
            \end{figure}
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Solução CQ}
    \begin{itemize}
        \item As células não tracejadas, após aplicar $\ut{W}$ viram
        \fixutspace
        \[ \text{center cell} = \left(\text{center cell} +
        \sum_{\text{8 nearest neighbors}}\text{cell}\right)/9.\]
        \pause
        \item Uma interpretação da matriz $\ut{W}$ é que
        $\ut{W}^2$ é a covariância \fixutspace \emph{a priori} do modelo. Assim, uma
        $\ut{W}$ como foi vista acima \fixutspace representa um modelo onde os dados de
        células próximas estão altamente correlacionadas.
        \pause
        \vfill\item Outra interpretação de $\ut{W}$ é que o raio é alargado, e passa
        a \fixutspace afetar os dados nas outras células.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Solução CQ}
    \begin{itemize}
        \item CQ \textbf{não} é equivalente a aplicar um suavizador na
            imagem final \cite{Peterson86}.
        \pause
        \vfill\item Primeiro, as colunas de $\ut{G}$ são suavizadas, e
        depois a imagem \fixutspace também o é.
        \pause
        \vfill\item A base teórica encontra-se no apêndice, em \cite{Backus70a}
        e \cite{Backus70b}.
    \end{itemize}
\end{frame}

\subsection{Figuras}
\begin{frame}
    \frametitle{Solução DLS}
    \begin{figure}[h]
        \includegraphics[height=6.5cm]{dls_solution.png}
        \caption{Solução DLS.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Modelo}
    \begin{figure}[h]
        \includegraphics[height=6.5cm]{model.png}
        \caption{Modelo.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Observações sobre a solução DLS}
    \begin{itemize}
        \item No meio, onde o recobrimento por raios é denso e uniforme, a
        imagem é boa.
        \vfill\item A região linear de baixa velocidade pode ser vista.
        \vfill\item A imagem é ruim perto da fronteira.
        \vfill\item Na direção das fontes são concentrados riscos escuros não
        presentes no modelo sintético.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Solução CQ}
    \begin{figure}[h]
        \includegraphics[height=6.5cm]{cq_solution.png}
        \caption{Solução CQ.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Modelo}
    \begin{figure}[h]
        \includegraphics[height=6.5cm]{model.png}
        \caption{Modelo.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Observações sobre a solução CQ}
    \begin{itemize}
        \item Como no outro, no meio, a imagem é boa.
        \vfill\item A região linear de baixa velocidade também pode ser vista.
        \vfill\item A imagem ainda apresenta os riscos escuros, mas são menos
        pronunciados.
        \vfill\item A supressão convolucional tem o efeito de alargar os riscos
        e torná-los menos prevalentes.
    \end{itemize}
\end{frame}

\subsection{\it{Resolving functions}}
\begin{frame}
    \frametitle{Solução DLS}
    \begin{figure}[h]
        \includegraphics[height=6.0cm]{dls_function.png}
        \caption{Solução DLS. Para a solução exata, a figura deveria ser um pico
        centrado no meio da região.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Solução CQ}
    \begin{figure}[h]
        \includegraphics[height=6.0cm]{cq_function.png}
        \caption{Solução CQ. Para a solução exata, a figura deveria ser a
        suavização por $\ut{W}$ de um pico centrado no meio da região.}
    \end{figure}
\end{frame}

\renewcommand{\theequation}{A-\arabic{equation}}
\setcounter{equation}{0}
\section{Apêndice}
\subsection{Supressão}
\begin{frame}
    \frametitle{Predição de funcionais lineares}
    \begin{itemize}
        \item  Considere o espaço de Hilbert $\ell^2$ (funções $\mathbb{R}^n
        \rightarrow \mathbb{R}$ quadrado integráveis).
        \pause
        \vfill\item Dados $G_i \in \ell^2$, $i=1,\ldots,M$, suponha
        que, temos, para um $m\in \ell^2$ desconhecido, as seguintes quantidades
        \begin{equation}\label{eq:data}
        d_i = \langle G_i, m \rangle.
        \end{equation}
        \pause
        \vfill\item Dado $P\in \ell^2$, o problema de predição de funcionais lineares consiste em
        estimar o valor de
        \begin{equation}\label{eq:predic}
        p = \langle P , m \rangle
        \end{equation}
        dados os $d_i$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Motivação}
    \begin{itemize}
        \item Backus e Gilbert (1968, 1970) mostram que a inversão tomográfica
        como fizemos é equivalente à predição de um conjunto de funções delta,
        já que
        \[m(\mathbf{r}_0) = \int_{\mathbb{R}^2} \delta(\mathbf{r} -
        \mathbf{r}_0)m(\mathbf{r}) \dif \mathbf{r} = \langle \delta(\mathbf{r}-\mathbf{r_0}),
        m(\mathbf{r})\rangle\]
        \pause
        \item $\langle \delta,\delta \rangle$ não é definido,
        logo $\delta \notin \ell^2$.
        \pause
        \vfill\item Como resolvemos este problema? Usando a técnica de
        supressão (\emph{quelling}).
    \end{itemize}
\end{frame}

\newcommand{\queller}{\mathbf{\Psi}}

\begin{frame}
    \frametitle{Supressão}
    \begin{itemize}
        \item Seja $\queller:X\rightarrow \ell^2$ um operador  que é Cauchy-contínuo.
        \\Um operador é Cauchy-contínuo quando uma sequencia no domínio é de Cauchy
        se e somente se a sua imagem é de Cauchy.
        %\\(Em $\ell^2$, Cauchy-continuidade é equivalente à continuidade).
        \pause
        \vfill\item $\queller$ tem duas propriedades importantes:
        \vfill
        \begin{enumerate}
            \item As quantidades $G_i^\prime = G_i\queller$ e $P^\prime =
            P\queller$ estão em $\ell^2$. Ou seja, $\queller$ suprime
            os $G_i$ e $P$.
            \pause
            \vfill\item $\queller$ tem uma única inversa
            $\queller^{-1}$ com as seguintes propriedades
            \begin{equation} \label{eq:datainv}
            \langle G_i\queller, \queller^{-1}m\rangle = \langle G_i, m\rangle
            \end{equation}
            \begin{equation}\label{eq:predinv}
            \langle P\queller, \queller^{-1}m\rangle = \langle P,
            m\rangle
            \end{equation}
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Supressão em operadores lineares}
    \begin{itemize}
        \item Definindo $m^\prime = \queller^{-1} m$, usando \ref{eq:datainv} and \ref{eq:predinv}, podemos
        escrever \ref{eq:data} e \ref{eq:predic} das seguinte formas
        \begin{equation}
        p = \langle P^\prime , m^\prime\rangle
        \label{eq:predicnew}
        \end{equation}
        \begin{equation}\label{eq:datanew}
        d_i = \langle G_i^\prime, m^\prime \rangle.
        \end{equation}
        \pause
        \vfill\item No artigo, foram usados modelos que são combinações lineares
        dos dados:
        \[\hat{m}^\prime = \sum_{i=1}^M a_i G_i^\prime\]
        Segue que $\displaystyle\hat{m} = \queller\hat{m}^\prime =
        \queller\sum_{i=1}^M a_i \queller G_i$.
    \end{itemize}
\end{frame}

\subsection{Supressão}
\begin{frame}
    \frametitle{Supressão por convolução}
    \begin{itemize}
        \item Como escolher um $\queller$ apropriado? Vamos usar uma convolução.
        $a\queller$ é definido por:
        \begin{equation}
        a\queller = a\ast s =
        \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} a(\xi,\eta)s(x-\xi,
        y-\eta) \dif\xi\dif\eta,
        \label{eq:conv}
        \end{equation}
        onde $s$ é um filtro suavizador.
        \pause
        \vfill\item A equação \ref{eq:datainv} vira
        \begin{equation}
        d_i = \langle G_i \ast s, m^\prime\rangle = \langle G_i^\prime,
        m^\prime\rangle
        \end{equation}
        Aqui, $m^\prime = s^{-1}\ast m$, onde $s^{-1}$ é o filtro inverso de $s$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Convolução discreta}
    \begin{itemize}
        \item A convolução é feita discretamente.
        \pause
        \vfill\item É possível mostrar que, discretizando a região, escrevendo
        $G_i^\prime$, $G_i$ e $s$ como matrizes $\ut{G}^\prime$, $\ut{G}$ e
        $\ut{W}$, respectivamente, a \fixutspace convolução \ref{eq:conv} é dada por
        \begin{equation}
        \ut{G}^\prime = \ut{G}\ut{W}
        \end{equation} 
    \end{itemize}
\end{frame}
\section{Referências}
\nocite{*}
\bibliography{apresentacao}{}
\bibliographystyle{apalike}
\end{document}
